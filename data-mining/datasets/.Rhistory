# ===============================================================
# Data Preprocessing and Exploration Script
# ===============================================================
# Author: [Your Name]
# Description:
# This script reads, merges, cleans, and analyzes four datasets
# related to data mining. It ensures consistency across datasets,
# handles missing values, and prepares the data for further analysis.
# ===============================================================
# -------------------------------
# 1. Set the Working Directory
# -------------------------------
# This sets the working directory to the location where the datasets are stored.
# Modify the path as per your system.
setwd("/home/feti/Documents/RProjects/data-mining/datasets")
# ===============================================================
# Data Preprocessing and Exploration Script
# ===============================================================
# Author: [Your Name]
# Description:
# This script reads, merges, cleans, and analyzes four datasets
# related to data mining. It ensures consistency across datasets,
# handles missing values, and prepares the data for further analysis.
# ===============================================================
# -------------------------------
# 1. Set the Working Directory
# -------------------------------
# This sets the working directory to the location where the datasets are stored.
# Modify the path as per your system.
setwd("/home/feti/Documents/RProjects/data-sciency-contents-with-R-and-Python/datasets")
# ===============================================================
# Data Preprocessing and Exploration Script
# ===============================================================
# Author: [Your Name]
# Description:
# This script reads, merges, cleans, and analyzes four datasets
# related to data mining. It ensures consistency across datasets,
# handles missing values, and prepares the data for further analysis.
# ===============================================================
# -------------------------------
# 1. Set the Working Directory
# -------------------------------
# This sets the working directory to the location where the datasets are stored.
# Modify the path as per your system.
setwd("/home/feti/Documents/RProjects/data-sciency-contents-with-R-and-Python/datasets")
# -------------------------------
# 1. Set the Working Directory
# -------------------------------
# This sets the working directory to the location where the datasets are stored.
# Modify the path as per your system.
setwd("/home/feti/Documents/RProjects/data-sciency-contents-with-R-and-Python/datasets")
# -------------------------------
# 1. Set the Working Directory
# -------------------------------
# This sets the working directory to the location where the datasets are stored.
# Modify the path as per your system.
setwd("/home/feti/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
cleveland_data <- read.csv('cleveland_data.txt', header = TRUE, sep =",", dec = ".")
hungary_data <- read.csv('hungary_data.txt', header = TRUE, sep =",", dec = ".")
long_beach_va_data <- read.csv('long-beach-va_data.txt', header = TRUE, sep =",", dec = ".")
switzerland_data <- read.csv('switzerland_data.txt', header = TRUE, sep =",", dec = ".")
# -------------------------------
# 3. Summarize Each Dataset
# -------------------------------
# The summary() function provides a statistical summary of each dataset.
summary(cleveland_data)
summary(hungary_data)
summary(long_beach_va_data)
summary(switzerland_data)
# -------------------------------
# 4. Add an Origin Identifier to Each Dataset
# -------------------------------
# This helps track the source of data after merging.
cleveland_data$origin <- 'cl'      # Cleveland
hungary_data$origin <- 'hu'        # Hungary
long_beach_va_data$origin <- 'lb'  # Long Beach VA
switzerland_data$origin <- 'sw'    # Switzerland
# -------------------------------
# 5. Standardize Column Names
# -------------------------------
# Ensure that all datasets have consistent column names.
# Example: Rename the 4th column in Switzerland data to "age" to match other datasets.
colnames(switzerland_data)[4] <- "age"
# -------------------------------
# 6. Data Cleaning - Fix Specific Columns
# -------------------------------
# Extract the second character from 'num' column in Hungary dataset
hungary_data$num <- substr(hungary_data$num, 2, 2)
# -------------------------------
# 7. Merge the Datasets
# -------------------------------
# Combine all datasets into one using rbind().
merged_data <- rbind(cleveland_data, long_beach_va_data, hungary_data, switzerland_data)
# -------------------------------
# 8. Create a Unique Identifier
# -------------------------------
# The ID column uniquely identifies each row.
merged_data$ID <- seq_len(nrow(merged_data))
# -------------------------------
# 9. Statistical Analysis
# -------------------------------
# Compute basic statistics for the 'age' column.
sd(merged_data$age)   # Standard deviation
mean(merged_data$age) # Mean age
min(merged_data$age)  # Minimum age
max(merged_data$age)  # Maximum age
summary(merged_data)  # Get an overall summary of the merged dataset
# -------------------------------
# 10. Identify Variable Types
# -------------------------------
# The str() function provides information about the structure of the dataset.
str(merged_data)
# Replace missing values represented as '?' with NA.
merged_data[merged_data == "?"] <- NA
# -------------------------------
# 11. Data Cleaning - Handling Categorical Variables
# -------------------------------
# Extract and fix 'thal' values for Cleveland data.
merged_data[merged_data$origin == "cl", "thal"] <- substr(merged_data[merged_data$origin == "cl", "thal"], 1, 1)
# Convert 'thal' column to a factor (categorical) variable.
merged_data$thal <- as.factor(merged_data$thal)
# Define a list of categorical variables.
categorical_variable_list <- c("origin", "sex", "cp", "fbs", "restecg", "exang", "slope", "num")
# Convert all categorical variables into factors.
merged_data[, categorical_variable_list] <- lapply(merged_data[, categorical_variable_list], as.factor)
# -------------------------------
# 12. Handle Missing Data
# -------------------------------
# Remove observations that have 3 or more missing values.
aux <- apply(merged_data, 1, function(i) sum(is.na(i)))
merged_data <- merged_data[aux <= 3, ]
# Display categorical variables to verify changes.
merged_data[, categorical_variable_list]
# ===============================================================
# Data Preprocessing and Exploration Script
# ===============================================================
# Author: [Your Name]
# Description:
# This script reads, merges, cleans, and analyzes four datasets
# related to data mining. It ensures consistency across datasets,
# handles missing values, and prepares the data for further analysis.
# ===============================================================
# -------------------------------
# 1. Set the Working Directory
# -------------------------------
# This sets the working directory to the location where the datasets are stored.
# Modify the path as per your system.
setwd("/home/feti/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
# -------------------------------
# 2. Load Required Datasets
# -------------------------------
# Read the CSV files containing data from different locations.
# 'header = T' ensures that the first row is treated as column names.
# 'sep = ","' specifies that the values are comma-separated.
# 'dec = "."' defines the decimal point format.
cleveland_data <- read.csv('cleveland_data.txt', header = TRUE, sep =",", dec = ".")
hungary_data <- read.csv('hungary_data.txt', header = TRUE, sep =",", dec = ".")
long_beach_va_data <- read.csv('long-beach-va_data.txt', header = TRUE, sep =",", dec = ".")
switzerland_data <- read.csv('switzerland_data.txt', header = TRUE, sep =",", dec = ".")
# -------------------------------
# 3. Summarize Each Dataset
# -------------------------------
# The summary() function provides a statistical summary of each dataset.
summary(cleveland_data)
summary(hungary_data)
summary(long_beach_va_data)
summary(switzerland_data)
# -------------------------------
# 4. Add an Origin Identifier to Each Dataset
# -------------------------------
# This helps track the source of data after merging.
cleveland_data$origin <- 'cl'      # Cleveland
hungary_data$origin <- 'hu'        # Hungary
long_beach_va_data$origin <- 'lb'  # Long Beach VA
switzerland_data$origin <- 'sw'    # Switzerland
# -------------------------------
# 5. Standardize Column Names
# -------------------------------
# Ensure that all datasets have consistent column names.
# Example: Rename the 4th column in Switzerland data to "age" to match other datasets.
colnames(switzerland_data)[4] <- "age"
# -------------------------------
# 6. Data Cleaning - Fix Specific Columns
# -------------------------------
# Extract the second character from 'num' column in Hungary dataset
hungary_data$num <- substr(hungary_data$num, 2, 2)
# -------------------------------
# 7. Merge the Datasets
# -------------------------------
# Combine all datasets into one using rbind().
merged_data <- rbind(cleveland_data, long_beach_va_data, hungary_data, switzerland_data)
# -------------------------------
# 8. Create a Unique Identifier
# -------------------------------
# The ID column uniquely identifies each row.
merged_data$ID <- seq_len(nrow(merged_data))
# -------------------------------
# 9. Statistical Analysis
# -------------------------------
# Compute basic statistics for the 'age' column.
sd(merged_data$age)   # Standard deviation
mean(merged_data$age) # Mean age
min(merged_data$age)  # Minimum age
max(merged_data$age)  # Maximum age
summary(merged_data)  # Get an overall summary of the merged dataset
# -------------------------------
# 10. Identify Variable Types
# -------------------------------
# The str() function provides information about the structure of the dataset.
str(merged_data)
# Replace missing values represented as '?' with NA.
merged_data[merged_data == "?"] <- NA
# -------------------------------
# 11. Data Cleaning - Handling Categorical Variables
# -------------------------------
# Extract and fix 'thal' values for Cleveland data.
merged_data[merged_data$origin == "cl", "thal"] <- substr(merged_data[merged_data$origin == "cl", "thal"], 1, 1)
# Convert 'thal' column to a factor (categorical) variable.
merged_data$thal <- as.factor(merged_data$thal)
# Define a list of categorical variables.
categorical_variable_list <- c("origin", "sex", "cp", "fbs", "restecg", "exang", "slope", "num")
# Convert all categorical variables into factors.
merged_data[, categorical_variable_list] <- lapply(merged_data[, categorical_variable_list], as.factor)
# -------------------------------
# 12. Handle Missing Data
# -------------------------------
# Remove observations that have 3 or more missing values.
aux <- apply(merged_data, 1, function(i) sum(is.na(i)))
merged_data <- merged_data[aux <= 3, ]
# Display categorical variables to verify changes.
merged_data[, categorical_variable_list]
View(merged_data)
View(hungary_data)
View(long_beach_va_data)
View(switzerland_data)
View(merged_data)
