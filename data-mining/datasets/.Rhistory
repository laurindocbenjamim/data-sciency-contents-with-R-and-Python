folder_indexes <- createFolds(df$RISK, k = 10)
# Display the structure of the dataset
str(df)
# Initialize an empty list to store accuracy
accuracy_list <- c()
prob_positive_class <- c()
accuracy_score_vec <- c()
precision_score_vec <- c()
recall_score_vec <- c()
f1_score_vec <- c()
auc_values <- c()
for(i in 1:10){
# creating training and testing sets
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model with the train set
model <- naiveBayes(RISK ~., data = train_set)
# Predict probabilities (type = "raw" gives probabilities for each class)
probs <- predict(model, test_set, type = "raw")
# Predict classes
predicted_classes <- predict(model, test_set)
# Calculte accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy_score)
# Focus on positive class probabilities (Assuming positive
# class is 'bad loss' or 'Positive')
if("0" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "0"])
}else if("1" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "1"])
}else{
stop("Could not find the positive class. Check column names in 'probs'.")
}
accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
}
# Calc decision-tree
d_tree_model <- rpart(RISK ~ ., data = df, method = "class")
# Load necessary libraries
library(caret)
library(readxl)
library(yardstick)
library(rpart)
library(pROC)
library(e1071)
# Set working directory (adjust if needed)
setwd("~/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
# Load the dataset
data <- read_excel("Classify_risk_dataset.xlsx")
df <- as.data.frame(data)
# Clean the target column
df$RISK <- trimws(df$RISK)            # Remove whitespace
df$RISK <- as.factor(df$RISK)         # Convert to factor
# Rename levels: bad loss = "1" (positive), good risk = "0" (negative)
levels(df$RISK) <- c("1", "0")        # bad loss first, good risk second
df$RISK <- relevel(df$RISK, ref = "0") # Set good risk as reference
# Check for missing values
cat("Missing values:\n")
print(colSums(is.na(df)))
# Remove any NA rows just in case
df <- na.omit(df)
# Set seed and create 10-fold CV
set.seed(123)
folder_indexes <- createFolds(df$RISK, k = 10)
# Initialize storage vectors
accuracy_list <- c()
precision_score_vec <- c()
recall_score_vec <- c()
f1_score_vec <- c()
auc_values <- c()
all_prob_positive_class <- c()
# Start cross-validation loop
for (i in 1:10) {
cat("\nFold", i, "\n")
# Train/test split
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model
model <- naiveBayes(RISK ~ ., data = train_set)
# Predict class labels and probabilities
predicted_classes <- predict(model, test_set)
probs <- predict(model, test_set, type = "raw")
# Extract probabilities for the positive class "1"
if ("1" %in% colnames(probs)) {
fold_probs <- probs[, "1"]
} else {
stop("Positive class '1' not found. Check column names in 'probs'.")
}
# Store all positive class probs if you want a global summary
all_prob_positive_class <- c(all_prob_positive_class, fold_probs)
# Accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy)
# Convert to tibble for yardstick
result_tbl <- data.frame(
truth = factor(test_set$RISK, levels = c("1", "0")),
prediction = factor(predicted_classes, levels = c("1", "0"))
)
# Metrics using yardstick
precision <- precision_vec(result_tbl, truth = truth, estimate = prediction)
recall <- recall_vec(result_tbl, truth = truth, estimate = prediction)
f1 <- f_meas_vec(result_tbl, truth = truth, estimate = prediction, beta = 1)
precision_score_vec <- c(precision_score_vec, precision)
recall_score_vec <- c(recall_score_vec, recall)
f1_score_vec <- c(f1_score_vec, f1)
# ROC and AUC
roc_curve <- roc(test_set$RISK, fold_probs, levels = c("1", "0"), direction = "<")
auc_values <- c(auc_values, auc(roc_curve))
# Optional: Plot ROC curve per fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = i != 1)
}
# Installing the libraries
#install.packages(e1071)
# importing the libraries
library(caret)
library(readxl)
library(yardstick)
library(rpart)
library(pROC)
library(e1071)
# Loading the dataset
setwd("~/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
data <- read_excel("Classify_risk_dataset.xlsx")
# Converting the dataset into a dataframe
df <- as.data.frame(data)
# Classifying the dependent variable/attribute
df$RISK <- trimws(dataset$RISK)
df$RISK <- as.factor(df$RISK)
levels(df$RISK) <- c("1", "0") # bad loss=1, good risk = 0
df$RISK <- relevel(df$RISK, ref = "0")
# Set seed for reproducibility
set.seed(123)
# Checking for missing values in dataset
cat("Checking for missing values in dataset: \n")
print(colSums(is.na(df)))
na.omit(df)
# Creating the 10-Cross validation to evaluate the performance of a model to predict the
# type of risk, using Na誰ve Bayes
folder_indexes <- createFolds(df$RISK, k = 10)
# Display the structure of the dataset
str(df)
# Initialize an empty list to store accuracy
accuracy_list <- c()
prob_positive_class <- c()
accuracy_score_vec <- c()
precision_score_vec <- c()
recall_score_vec <- c()
f1_score_vec <- c()
auc_values <- c()
for(i in 1:10){
# creating training and testing sets
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model with the train set
model <- naiveBayes(RISK ~., data = train_set)
# Predict probabilities (type = "raw" gives probabilities for each class)
probs <- predict(model, test_set, type = "raw")
# Predict classes
predicted_classes <- predict(model, test_set)
# Calculte accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy_score)
# Focus on positive class probabilities (Assuming positive
# class is 'bad loss' or 'Positive')
if("0" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "0"])
}else if("1" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "1"])
}else{
stop("Could not find the positive class. Check column names in 'probs'.")
}
accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
}
library(readxl)
library(yardstick)
library(rpart)
library(pROC)
library(e1071)
# Loading the dataset
setwd("~/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
data <- read_excel("Classify_risk_dataset.xlsx")
df <- as.data.frame(data)
# Classifying the dependent variable/attribute
df$RISK <- trimws(dataset$RISK)
df$RISK <- as.factor(df$RISK)
levels(df$RISK) <- c("1", "0") # bad loss=1, good risk = 0
df$RISK <- relevel(df$RISK, ref = "0")
# Set seed for reproducibility
set.seed(123)
# Checking for missing values in dataset
cat("Checking for missing values in dataset: \n")
print(colSums(is.na(df)))
na.omit(df)
# Creating the 10-Cross validation to evaluate the performance of a model to predict the
# type of risk, using Na誰ve Bayes
folder_indexes <- createFolds(df$RISK, k = 10)
# Display the structure of the dataset
str(df)
# Initialize an empty list to store accuracy
accuracy_list <- c()
prob_positive_class <- c()
accuracy_score_vec <- c()
precision_score_vec <- c()
recall_score_vec <- c()
f1_score_vec <- c()
auc_values <- c()
for(i in 1:10){
# creating training and testing sets
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model with the train set
model <- naiveBayes(RISK ~., data = train_set)
# Predict probabilities (type = "raw" gives probabilities for each class)
probs <- predict(model, test_set, type = "raw")
# Predict classes
predicted_classes <- predict(model, test_set)
# Calculte accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy_score)
# Focus on positive class probabilities (Assuming positive
# class is 'bad loss' or 'Positive')
if("0" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "0"])
}else if("1" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "1"])
}else{
stop("Could not find the positive class. Check column names in 'probs'.")
}
if ("1" %in% colnames(probs)) {
prob_positive_class <- probs[, "1"]
} else {
stop("Positive class '1' not found. Check column names in 'probs'.")
}
#accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
accuracy_score_vec[i] <- accuracy_vec(truth = test_set$RISK, estimate = predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
}
# Compute meac auc
mean_auc <- mean(auc_values)
# Calc decision-tree
d_tree_model <- rpart(RISK ~ ., data = df, method = "class")
# Accuracy
cat("Average accuracy accross 10 folds: ", mean(accuracy_list), "\n")
# Summary of positive class probabilities
cat("Mean Probability of positive class: ", mean(prob_positive_class), "\n")
cat("Accuracy score: ", mean(accuracy_score_vec), "\n")
cat("Precision score: ", mean(precision_score_vec), "\n")
cat("Recall score: ", mean(recall_score_vec), "\n")
cat("F-1 Score: ", mean(f1_score_vec), "\n")
cat("Mean AUC: ", mean_auc)
# plot the decision tree
plot(d_tree_model)
text(d_tree_model, use.n = TRUE, cex = 0.8)
# Installing the libraries
#install.packages(e1071)
# importing the libraries
library(caret)
library(readxl)
library(yardstick)
library(rpart)
library(pROC)
library(e1071)
# Loading the dataset
setwd("~/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
data <- read_excel("Classify_risk_dataset.xlsx")
# Converting the dataset into a dataframe
df <- as.data.frame(data)
# Classifying the dependent variable/attribute
df$RISK <- trimws(dataset$RISK)
df$RISK <- as.factor(df$RISK)
levels(df$RISK) <- c("1", "0") # bad loss=1, good risk = 0
df$RISK <- relevel(df$RISK, ref = "0")
# Set seed for reproducibility
set.seed(123)
# Checking for missing values in dataset
cat("Checking for missing values in dataset: \n")
print(colSums(is.na(df)))
na.omit(df)
# Creating the 10-Cross validation to evaluate the performance of a model to predict the
# type of risk, using Na誰ve Bayes
folder_indexes <- createFolds(df$RISK, k = 10)
# Display the structure of the dataset
str(df)
# Initialize an empty list to store accuracy
accuracy_list <- c()
prob_positive_class <- c()
accuracy_score_vec <- c()
precision_score_vec <- c()
recall_score_vec <- c()
f1_score_vec <- c()
auc_values <- c()
for(i in 1:10){
# creating training and testing sets
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model with the train set
model <- naiveBayes(RISK ~., data = train_set)
# Predict probabilities (type = "raw" gives probabilities for each class)
probs <- predict(model, test_set, type = "raw")
# Predict classes
predicted_classes <- predict(model, test_set)
# Calculte accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy_score)
# Focus on positive class probabilities (Assuming positive
# class is 'bad loss' or 'Positive')
if("0" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "0"])
}else if("1" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "1"])
}else{
stop("Could not find the positive class. Check column names in 'probs'.")
}
if ("1" %in% colnames(probs)) {
prob_positive_class <- probs[, "1"]
} else {
stop("Positive class '1' not found. Check column names in 'probs'.")
}
#accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
accuracy_score_vec[i] <- accuracy_vec(truth = test_set$RISK, estimate = predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
}
# Compute meac auc
mean_auc <- mean(auc_values)
# Calc decision-tree
d_tree_model <- rpart(RISK ~ ., data = df, method = "class")
# Accuracy
cat("Average accuracy accross 10 folds: ", mean(accuracy_list), "\n")
# Summary of positive class probabilities
cat("Mean Probability of positive class: ", mean(prob_positive_class), "\n")
cat("Accuracy score: ", mean(accuracy_score_vec), "\n")
cat("Precision score: ", mean(precision_score_vec), "\n")
cat("Recall score: ", mean(recall_score_vec), "\n")
cat("F-1 Score: ", mean(f1_score_vec), "\n")
cat("Mean AUC: ", mean_auc)
# plot the decision tree
plot(d_tree_model)
text(d_tree_model, use.n = TRUE, cex = 0.8)
# Loading the dataset
setwd("~/Documents/RProjects/data-sciency-contents-with-R-and-Python/data-mining/datasets")
data <- read_excel("Classify_risk_dataset.xlsx")
df <- as.data.frame(data)
# Classifying the dependent variable/attribute
df$RISK <- trimws(dataset$RISK)
df$RISK <- as.factor(df$RISK)
levels(df$RISK) <- c("1", "0") # bad loss=1, good risk = 0
df$RISK <- relevel(df$RISK, ref = "0")
# Set seed for reproducibility
set.seed(123)
# Checking for missing values in dataset
cat("Checking for missing values in dataset: \n")
print(colSums(is.na(df)))
na.omit(df)
# Creating the 10-Cross validation to evaluate the performance of a model to predict the
# type of risk, using Na誰ve Bayes
folder_indexes <- createFolds(df$RISK, k = 10)
# Display the structure of the dataset
str(df)
# Initialize an empty list to store accuracy
accuracy_list <- c()
prob_positive_class <- c()
accuracy_score_vec <- c()
precision_score_vec <- c()
recall_score_vec <- c()
f1_score_vec <- c()
auc_values <- c()
for(i in 1:10){
# creating training and testing sets
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model with the train set
model <- naiveBayes(RISK ~., data = train_set)
# Predict probabilities (type = "raw" gives probabilities for each class)
probs <- predict(model, test_set, type = "raw")
# Predict classes
predicted_classes <- predict(model, test_set)
# Calculte accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy_score)
# Focus on positive class probabilities (Assuming positive
# class is 'bad loss' or 'Positive')
if("0" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "0"])
}else if("1" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "1"])
}else{
stop("Could not find the positive class. Check column names in 'probs'.")
}
if ("1" %in% colnames(probs)) {
prob_positive_class <- probs[, "1"]
} else {
stop("Positive class '1' not found. Check column names in 'probs'.")
}
#accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
accuracy_score_vec[i] <- accuracy_vec(truth = test_set$RISK, estimate = predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
}
# Compute meac auc
mean_auc <- mean(auc_values)
# Calc decision-tree
d_tree_model <- rpart(RISK ~ ., data = df, method = "class")
#accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
accuracy_score_vec[i] <- accuracy_vec(truth = test_set$RISK, estimate = predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
for(i in 1:10){
# creating training and testing sets
test_indexes <- folder_indexes[[i]]
train_set <- df[-test_indexes, ]
test_set <- df[test_indexes, ]
# Train Naive Bayes model with the train set
model <- naiveBayes(RISK ~., data = train_set)
# Predict probabilities (type = "raw" gives probabilities for each class)
probs <- predict(model, test_set, type = "raw")
# Predict classes
predicted_classes <- predict(model, test_set)
# Calculte accuracy
accuracy <- mean(predicted_classes == test_set$RISK)
accuracy_list <- c(accuracy_list, accuracy_score)
# Focus on positive class probabilities (Assuming positive
# class is 'bad loss' or 'Positive')
if("0" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "0"])
}else if("1" %in% colnames(probs)){
prob_positive_class <- c(prob_positive_class, probs[, "1"])
}else{
stop("Could not find the positive class. Check column names in 'probs'.")
}
if ("1" %in% colnames(probs)) {
prob_positive_class <- probs[, "1"]
} else {
stop("Positive class '1' not found. Check column names in 'probs'.")
}
#accuracy_score_vec[i] <- accuracy_vec(test_set$RISK, predicted_classes)
accuracy_score_vec[i] <- accuracy_vec(truth = test_set$RISK, estimate = predicted_classes)
precision_score_vec[i] <- precision_vec(test_set$RISK, predicted_classes)
recall_score_vec[i] <- recall_vec(test_set$RISK, predicted_classes)
f1_score_vec[i] <- f_meas_vec(test_set$RISK, predicted_classes, beta = 1)
# Computing the ROK curve and AUC for each attribute
roc_curve <- roc(test_set$RISK, prob_positive_class, levels = c("1", "0"), direction = "<")
auc_values[i] <- auc(roc_curve)
# Plot roc curve for this fold
plot.roc(roc_curve, col = rgb(0.2, 0.6, 0.8, alpha = 0.5), add = TRUE)
}
# Compute meac auc
mean_auc <- mean(auc_values)
# Calc decision-tree
d_tree_model <- rpart(RISK ~ ., data = df, method = "class")
# Accuracy
cat("Average accuracy accross 10 folds: ", mean(accuracy_list), "\n")
# Summary of positive class probabilities
cat("Mean Probability of positive class: ", mean(prob_positive_class), "\n")
cat("Accuracy score: ", mean(accuracy_score_vec), "\n")
cat("Precision score: ", mean(precision_score_vec), "\n")
cat("Recall score: ", mean(recall_score_vec), "\n")
cat("F-1 Score: ", mean(f1_score_vec), "\n")
cat("Mean AUC: ", mean_auc)
# plot the decision tree
plot(d_tree_model)
text(d_tree_model, use.n = TRUE, cex = 0.8)
View(probs)
View(probs)
View(roc_curve)
View(tree_model)
